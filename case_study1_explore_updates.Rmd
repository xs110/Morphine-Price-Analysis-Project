---
title: "Case Study 1 - Explore"
author: "Weiting Miao"
date: "2023-10-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = 'center')
```



```{r, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(car)
library(lme4)
library(lmerTest)
library(lattice)
library(merTools)
library(gridExtra)
library(mice)
library(kableExtra)
library(influence.ME)
```

```{r}
#load data
load("streetrx.RData")
df <- streetrx %>%
  filter(api_temp == "morphine", !is.na(ppm))
```

### Questions of Interest
To investigate factors related to the price per mg of your drug, accounting for potential clustering by location and exploring heterogeneity in pricing by location.

### Data Cleaning

Variables:

- Outcome variable: ppm
- Purchasing time: year, month, day, quarter
- Location: country, state, city, USA_region  - country: all US; city: many missing
- Source: source of information               - Unknown
- Formulation of the drug: factor             - no variation, drop it
- Dosage strength                             - whether treat it as factor or numeric variable, or group them
- Bulk Purchase: dummy
- Primary reason: factor                      - many missing values

(1). Drop observations if the outcome variable is missing


```{r}
# log ppm
df$log_ppm <- log(df$ppm)

hist_ppm <- ggplot(df, aes(x=ppm)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white", bins = 50)

hist_log_ppm <- ggplot(df, aes(x=log_ppm)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white", bins = 50)
grid.arrange(hist_ppm, hist_log_ppm, ncol=2)

df <- df %>% filter(ppm > 0)
```


(2). Drop variables without any variation - country, formulation

(3). Remove states with 1 observation, otherwise, we cannot estimate the within-group variance for these states. State has 1 missing values, and it has wrong category "USA". Since our goal is to understand the heterogeneity across locations, we remove observations with "USA" as the state name. City has a lot of missing values, so we will focus on the heterogeneity across states.

```{r}
result <- df %>%
  group_by(state) %>%
  summarise(Count = n()) %>%
  arrange(-Count)  # Sorting in descending order

# Print the sorted table
kable(result)


# remove states with 1 observation
df <- df %>%
  group_by(state) %>%
  mutate(count = n()) %>%
  filter(count > 1) 

# state
df <- df%>%
  mutate(state = case_when(
    state == "USA" ~ "Unknown",
    is.na(state) ~ "Unknown",
    TRUE ~ state
  ))

# df <- df%>%filter(state!="Unknown")
```


```{r}
result <- df %>%
  group_by(USA_region) %>%
  summarise(Count = n()) %>%
  arrange(-Count)  # Sorting in descending order

# Print the sorted table
kable(result)
```

```{r}
df <- df %>%
  mutate(state = as.factor(state),
         USA_region = as.factor(USA_region))

```


(4). Create year and quarter variables. Very limited observations before year 2010. Given the nonlinear effects of year and quarter, we consider to treat year and quarter as factor variables.

```{r}
df <- df %>%
  mutate(price_date = mdy(price_date),
         year = year(price_date))

df$quarter = as.numeric(substr(df$yq_pdate, nchar(df$yq_pdate), nchar(df$yq_pdate)))

df <- df %>%
  mutate(year_factor = as.factor(year),
         quarter_factor = as.factor(quarter))


eda_year = ggplot(data = df %>% filter(year>2009), aes(x = year, y = log_ppm)) +
  geom_point(size = 0.5, alpha = 0.5) +   
  geom_line(size = 0.5, alpha = 0.2) +
  geom_smooth( method = "loess", se = FALSE, size = 1.5) + 
  labs(x = "Year",
       y = "Log(ppm)")

eda_quarter = ggplot(data = df %>% filter(year>2009), aes(x = quarter, y = log_ppm)) +
  geom_point(size = 0.5, alpha = 0.5) +   
  geom_line(size = 0.5, alpha = 0.2) +
  geom_smooth( method = "loess", se = FALSE, size = 1.5) + 
  labs(x = "Quarter",
       y = "Log(ppm)")

grid.arrange(eda_year, eda_quarter, ncol=2)

```

```{r}
histogram(~log_ppm|quarter_factor,data = df)
```

Quarter doesn't seem to be important.


(5). Source: from the raw data, most can be coded as "Internet".

```{r}
result <- df %>%
  group_by(source) %>%
  summarise(Count = n()) %>%
  arrange(-Count)  # Sorting in descending order

# Print the sorted table
kable(result)
```

```{r}
df$source <- as.character(df$source)
df$source <- ifelse(grepl("^http", df$source), "Internet", df$source)


df <- df %>%
  mutate(source_group = case_when(
    source == "" ~ "Unknown",
    source == "Heard it" ~ "Heard it",
    source == "Personal" ~ "Personal",
    source == "Internet Pharmacy" ~ "Internet Pharmacy",
    source == "L" ~  "Unknown",
    TRUE ~ "Internet"
  ))

source_ppm <- ggplot(df, aes(x = source_group, y = log(ppm))) + 
    geom_boxplot(aes(fill = source_group), outlier.size = 0.1) + 
  labs(x = "Source") + 
  theme(legend.position = "none") +
  coord_flip()
source_ppm
```

(6). Dosage strength: should we group them?

```{r}
table(df$mgstr)

df <- df %>%
  mutate(mgstr = as.numeric(mgstr),
         mgstr_factor = as.factor(mgstr))

mgstr_ppm_factor <- ggplot(df, aes(x = factor(mgstr), y = log(ppm))) + 
    geom_boxplot(aes(fill = factor(mgstr)), outlier.size = 0.1) + 
  labs(x = "mgstr") + 
  theme(legend.position = "none") +
  coord_flip()

mgstr_ppm = ggplot(data = df%>%filter(mgstr > 5), aes(x = mgstr, y = log_ppm)) +
  geom_point(size = 0.5, alpha = 0.5) +   
  geom_line(size = 0.5, alpha = 0.2) +
  geom_smooth( method = "loess", se = FALSE, size = 1.5) + 
  labs(x = "mgstr",
       y = "Log(ppm)")

grid.arrange(mgstr_ppm_factor, mgstr_ppm, ncol=2)
```
The effects are mostly monotone and linear except for when mgstr < 10 (5 obs).

(7). Bulk Purchase

```{r}
df <- df %>%
  mutate(bulk_purchase = as.factor(bulk_purchase))

bp_ppm <- ggplot(df, aes(x = bulk_purchase, y = log(ppm))) + 
    geom_boxplot(aes(fill = bulk_purchase), outlier.size = 0.1) + 
  labs(x = "Bulk Purchase") + 
  theme(legend.position = "none") + 
  coord_flip()

bp_ppm
```

(8). Primary reason: 70% of the observations are NA or didn't answer the question. We decide not to use this variable.
```{r}
result <- df %>%
  group_by(Primary_Reason) %>%
  summarise(Count = n()) %>%
  arrange(-Count)  # Sorting in descending order

# Print the sorted table
kable(result)
```




### Model Building & Model Selection

Predictors: year, quarter, mgstr, bulk_purchase, source
Random intercept: state
Random slope: 

(1) Random effect or fixed effect?
```{r}
m1 <- lm(formula = log_ppm ~ year + quarter_factor +  mgstr + bulk_purchase + source + state, data=df)
m2 <- lmer(formula = log_ppm ~ year + quarter_factor + mgstr + bulk_purchase + source + (1 | state), data=df)
BIC(m1)
BIC(m2)
```

Random effect model has much smaller BIC

(2) Year as numeric variable or factor variable?
```{r}
m2 <- lmer(formula = log_ppm ~ year + quarter_factor  + mgstr + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m3 <- lmer(formula = log_ppm ~ year_factor + quarter_factor  +  mgstr + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m2,m3)
```
Treat it as numerical variable

(3) mgstr as numeric variable or factor variable?
```{r}
m2 <- lmer(formula = log_ppm ~ year + quarter_factor  + mgstr + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m4 <- lmer(formula = log_ppm ~ year + quarter_factor  +  mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m2,m4)
```
Treat mgstr as factor variable


(4) Do we need quarter?
```{r}
m4 <- lmer(formula = log_ppm ~ year + quarter_factor  +  mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m5 <- lmer(formula = log_ppm ~ year  +  mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m4,m5)
```
Quarter doesn't seem to be important. Drop quarter.

(5) Do we need year?

```{r}
m5 <- lmer(formula = log_ppm ~ year  +  mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m6 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m5,m6)
```
Year doesn't seem to be important. Drop year.

(6) Do we need source?

```{r}
m6 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + source + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m7 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m6, m7)
```
Adding source into the regression increases BIC, get rid of source


(7) Any random slope

```{r}
m7 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m8 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + (1 + bulk_purchase | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m7, m8)
```

Having random slope of bulk purchase doesn't improve the performance


(8) Any interaction term

```{r}
m7 <- lmer(formula = log_ppm ~ mgstr_factor + bulk_purchase + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
m9 <- lmer(formula = log_ppm ~ mgstr_factor * bulk_purchase + (1 | state), data=df %>% filter(year>2009 & state!="Unknown"))
anova(m7, m9)
```
Not helpful to include the interaction term.

The final model is m7.

```{r}
## any systematic way to check the model performance of different combination?
```

```{r}
dotplot(ranef(m7,condVar=TRUE), scales = list(x = list(relation = 'free')))$state
```


To do:

1. Missing data imputation + Adding source into the model and compare BIC
2. Outliers or influential points
3. Model diagnostics
4. Model interpretation + Results visualization









## Missing data imputation

### preprocess data
```{r}
# If df == "", them map to NA
# Replace empty strings with NA
df[df == ""] <- NA
md.pattern(df,rotate.names = TRUE)
```
At the bottom: total number of missing values by variables.
On the right: number of variables missing in each pattern.
On the left: number of cases for each pattern.

Missing data:
State has 1 missing values, and it has wrong category "USA", so we also map to unknown
City has 3126 missing values and duplicate entries as well as abbreviations, so we need to preprocess city(or just drop it). 
source has 3695 missing values and messy website.
Primary_Reason has 4754 missing values # drop? over 60% missing

Next step: preprocess these columns then use mice to impute missing values.

```{r}
result <- df %>%
  group_by(source) %>%
  summarise(Count = n()) %>%
  arrange(-Count)  # Sorting in descending order

# Print the sorted table
kable(result)
```
```{r}
#city
# # briefly preprocess data for data with over 4 counts.
# # Convert city names to title case
# df$city <- str_to_title(df$city)
# 
# # Create a mapping for abbreviations and other variations
# corrections <- data.frame(
#   original = c("Phx", "Dallas - Fort Worth", "philadelphia Pa", "Sacramento california", "Los Angeles California", "Nashville tn", "San Francisco California City", "saint Louis", "Sf"),
#   corrected = c("Phoenix", "Dallas", "Philadelphia", "Sacramento", "Los Angeles", "Nashville", "San Francisco", "Saint Louis", "San Francisco")
# )
# 
# # Use the mapping to replace city names
# df <- df %>%
#   left_join(corrections, by = c("city" = "original")) %>%
#   mutate(city = ifelse(is.na(corrected), city, corrected))

#drop city
df <- subset(df, select = -city)

```


```{r}
# state
df <- df%>%
  mutate(state = case_when(
    state == "USA" ~ "Unknown",
    is.na(state) ~ "Unknown",
    TRUE ~ state
  ))
```

```{r}
# source
df$source <- as.character(df$source)
df$source <- ifelse(grepl("^http", df$source), "Internet", df$source)

freq_df_source <- as.data.frame(table(df$source))
colnames(freq_df_source) <- c("Value", "Frequency")
freq_df_source[freq_df_source$Frequency > 50, ]

df <- df %>%
  mutate(source = case_when(
    # is.null(source)  ~ "Unknown", ## doesn't work
    # is.na(source)  ~ "Unknown",
    # source == "" ~ "Unknown",
    source == "Heard it" ~ "Heard it",
    source == "Personal" ~ "Personal",
    source == "Internet Pharmacy" ~ "Internet Pharmacy",
    !is.na(source) & source != "Personal" & source != "Heard it" & source != "Internet Pharmacy" ~ "Internet",
    source == "L" ~ NA,
    TRUE ~ source
  ))


table(df$form_temp) ## almost all are pill/tablet -> not include it in the model
table(df$mgstr) ## whether treat it as factor or numeric variable, or group them
table(df$bulk_purchase)
table(df$Primary_Reason) ## many missing values -> not include it in the model

# remove states with 1 observation
df <- df %>%
  group_by(state) %>%
  mutate(count = n()) %>%
  filter(count > 1) 
table(df$state)


# code factors and numeric variables 
df <- df %>%
  mutate(ppm = as.numeric(ppm), 
         price_date = mdy(price_date),
         year = year(price_date),
         state = as.factor(state),
         country = as.factor(country),
         USA_region = as.factor(USA_region),
         source = as.factor(source),
         mgstr = as.numeric(mgstr),
         bulk_purchase = as.factor(bulk_purchase)
         )

df$quarter = as.numeric(substr(df$yq_pdate, nchar(df$yq_pdate), nchar(df$yq_pdate)))
```


### year and quarter

Do we need to include year or quarter?

```{r}
eda_year = ggplot(data = df %>% filter(year>2009), aes(x = year, y = log_ppm)) +
  geom_point(size = 0.5, alpha = 0.5) +   
  geom_line(size = 0.5, alpha = 0.2) +
  geom_smooth( method = "loess", se = FALSE, size = 1.5) + 
  labs(x = "Year",
       y = "Log(ppm)")

eda_quarter = ggplot(data = df %>% filter(year>2009), aes(x = quarter, y = log_ppm)) +
  geom_point(size = 0.5, alpha = 0.5) +   
  geom_line(size = 0.5, alpha = 0.2) +
  geom_smooth( method = "loess", se = FALSE, size = 1.5) + 
  labs(x = "Quarter",
       y = "Log(ppm)")

grid.arrange(eda_year, eda_quarter, ncol=2)
```

Year effect is not linear. Should consider year as a factor variable.

Treat year as factor
```{r}
# year 
# observations before 2010 are bare
df <- df %>% filter(year>=2010)
df$year <- as.factor(df$year)

#quarter

df$quarter <- as.factor(df$quarter)
```

```{r}
table(df$source)
table(df$state)
table(df$USA_region)
```



### Imputation

We assume that the data is MAR, the next step is to use MICE to impute missing data.

One problem: city has over 50 categories and is messed up with duplicate and abbreviation, so here we will drop city and focus on factor variables: state, USA_region

```{r}
library(VIM)
aggr(df,col=c("lightblue3","darkred"),numbers=TRUE,sortVars=TRUE,
labels=names(df),cex.axis=.7,gap=3,
ylab=c("Proportion missing","Missingness pattern"))
```
The typical sequence of steps to perform a multiple imputation analysis is:

1) Impute the missing data by the mice() function, resulting in a multiple imputed data set (class mids);

2) Fit the model of interest (scientific model) on each imputed data set by the with() function, resulting an object of class mira;

3) Pool the estimates from each model into a single set of estimates and standard errors, resulting in an object of class mipo;

4) Optionally, compare pooled estimates from different scientific models by the D1() or D3() functions.
```{r}
# multiple impute the missing values
imp <- mice(df,m =5, seed = 1)
```

### model selection

The standard multiple imputation scheme consists of three phases:

1. Imputation of the missing data m = 5 times;

2. Analysis of the m = 5 imputed datasets;

3. Pooling of the parameters across m = 5 analyses.





```{r}
fit.without <- with(imp, lm(log_ppm ~ year +  mgstr + bulk_purchase + source))
fit.with <- with(imp, lm(log_ppm ~ year +  quarter + mgstr + bulk_purchase + source))
D3(fit.with, fit.without)
```
The  p-value is equal to 0.1354, so quarter is not needed in the model.

```{r}
summary(pool(fit.without))
```
```{r}
histogram(~log_ppm|state,data = df)
```
need random effect




```{r}
### random intercept
state_ppm <- ggplot(df, aes(x=log_ppm, y=reorder(state, state, length))) +
  geom_boxplot(outlier.size = 0.1) +
  labs(title="Log(ppm) by State", x = "Log(ppm)", y = "State")

usaregion_ppm <- ggplot(df, aes(x=log_ppm, y=reorder(USA_region, USA_region, length))) +
  geom_boxplot(outlier.size = 0.1) +
  labs(title="Log(ppm) by USA_region", x = "Log(ppm)", y = "USA_region")

state_ppm
usaregion_ppm
```

use state as random intercept

```{r}
#source
fit <- with(imp, anova(lmer(log_ppm ~ year + mgstr + quarter + bulk_purchase  + (1|state)),
                               lmer(log_ppm ~ year + mgstr + quarter + bulk_purchase + source + (1|state))))

summary(fit)
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

```


```{r}
#quarter
fit <- with(imp, anova(lmer(log_ppm ~ year + mgstr + quarter + bulk_purchase + source + (1|state)),
                               lm(log_ppm ~ year + mgstr + quarter + bulk_purchase + source + state)))

summary(fit)
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

```

go random effect

```{r}
#quarter
fit <- with(imp, anova(lmer(log_ppm ~ year +  mgstr + bulk_purchase + source + (1|state)),
                               lmer(log_ppm ~ year + mgstr + quarter + bulk_purchase + source + (1|state))))

summary(fit)
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

```
drop quarter

```{r}
#USA_region VS. state
fit <- with(imp, anova(lmer(log_ppm ~ year +  mgstr + bulk_purchase + source + (1|state)),
                               lmer(log_ppm ~ year + mgstr  + bulk_purchase + source + (1|USA_region))))
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

```
use state as random intercept
```{r}
fit <- with(imp, anova(lmer(log_ppm ~  mgstr + bulk_purchase + source + (1|state)),
                               lmer(log_ppm ~ year + mgstr  + bulk_purchase + source + (1|state))))


#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

summary(fit)

```
Do we need year factor?
Drop year




```{r}
df$year = as.factor(df$year)
df$quarter = as.factor(df$quarter)

mgstr_ppm <- ggplot(df, aes(x = factor(mgstr), y = log(ppm))) + 
    geom_boxplot(aes(fill = factor(mgstr)), outlier.size = 0.1) + 
  labs(x = "mgstr") + 
  theme(legend.position = "none") +
  coord_flip()

bp_ppm <- ggplot(df, aes(x = bulk_purchase, y = log(ppm))) + 
    geom_boxplot(aes(fill = bulk_purchase), outlier.size = 0.1) + 
  labs(x = "Bulk Purchase") + 
  theme(legend.position = "none") + 
  coord_flip()

source_ppm <- ggplot(df, aes(x = source, y = log(ppm))) + 
    geom_boxplot(aes(fill = source), outlier.size = 0.1) + 
  labs(x = "Source") + 
  theme(legend.position = "none") +
  coord_flip()


grid.arrange(mgstr_ppm, bp_ppm, source_ppm, ncol=2)
```

Might have interaction effect..

```{r}
ggplot(df, aes(x = mgstr, y = log(ppm))) + 
  geom_point() +
  geom_smooth( method = "loess") +
  facet_wrap(~bulk_purchase)


ggplot(df, aes(x = mgstr, y = log(ppm))) + 
  geom_point() +
  geom_smooth( method = "loess") +
  facet_wrap(~quarter)

ggplot(df, aes(x = bulk_purchase, y = log(ppm))) + 
  geom_boxplot() +
  facet_wrap(~quarter)
```

```{r}
fit <- with(imp, anova(lmer(log_ppm ~    mgstr+bulk_purchase +  source + (1+mgstr|state) ),
                               lmer(log_ppm ~   mgstr  + bulk_purchase + source + (1|state))))
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))

summary(fit)
```
```{r}
fit <- with(imp, anova(lmer(log_ppm ~    mgstr+bulk_purchase +  source + (1+bulk_purchase+mgstr|state) ),
                               lmer(log_ppm ~   mgstr  + bulk_purchase + source + (1+mgstr|state) )))
#AIC
mean(summary(fit)$AIC[c(1,3,5,7,9)])
mean(summary(fit)$AIC[c(2,4,6,8,10)])

#BIC
print(mean(summary(fit)$BIC[c(1,3,5,7,9)]))
print(mean(summary(fit)$BIC[c(2,4,6,8,10)]))
summary(fit)
```
```{r}
table(df$mgstr)
```
```{r}
ggplot(df, aes(x = mgstr, y = log(ppm))) + 
  geom_point() +
  geom_smooth( method = "loess")
```
```{r}
df$mgstr_factor <- as.factor(df$mgstr)
```

```{r}
# mgstr factor
m1 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1  | state), data=df)
m2 <- lmer(formula = log_ppm ~ year +  mgstr_factor + bulk_purchase + source + (1  | state), data=df)

anova(m1,m2)
```
```{r}
# source 

df <- df %>%
  mutate(source = case_when(
    is.na(source)  ~ "Unknown",
    TRUE ~ source
  ))



m3 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + (1  | state), data=df)
m4 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1  | state), data=df)

anova(m3,m4)
```

### Model Building & Model Selection

Predictors: year, quarter, mgstr, bulk_purchase, source, 
Random intercept: state
Random slope: ? state by 

```{r}
## whether to include quarter

m1 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 | state), data=df)
m2 <- lmer(formula = log_ppm ~ year + quarter + mgstr + bulk_purchase + source + (1 | state), data=df)
anova(m1,m2)
```
```{r}
## whether to include interaction terms

m1 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 | state), data=df)
m3 <- lmer(formula = log_ppm ~ year +  mgstr*bulk_purchase + source + (1 | state), data=df)
anova(m1,m3)
```
```{r}
## whether to include random slope

m1 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 | state), data=df)
m4 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 +  mgstr | state), data=df)
anova(m1,m4)
```

```{r}
## whether to include more random slope - No

m4 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 +  mgstr | state), data=df)
m5 <- lmer(formula = log_ppm ~ year +  mgstr + bulk_purchase + source + (1 +  mgstr + bulk_purchase| state), data=df)

anova(m4,m5)
```
```{r}
## any systematic way to check the model performance of different combination?
```

### Model Diagnostics

```{r}
plot(m4)

residual <- resid(m4)

ggplot() + geom_density(aes(x = residual))

```




